# -*- coding: utf-8 -*-
"""
AIåŽå¤„ç†å™¨ - ä½¿ç”¨LLMæ™ºèƒ½åŽ»é‡å’Œåˆ†æžæ£€æµ‹ç»“æžœ
"""
import json
from typing import Dict, List, Any
from pathlib import Path
from tools.llm_client import LLMClient
from tools.code_extractor import CodeExtractor
from utils.logger import log_info, log_error, log_warning
class AIPostProcessor:
    """AIé©±åŠ¨çš„æ£€æµ‹ç»“æžœåŽå¤„ç†å™¨"""
    
    def __init__(self, llm_client: LLMClient = None):
        self.llm_client = llm_client or LLMClient()
        self.code_extractor = CodeExtractor()
        
    async def process_detection_results(
        self,
        raw_results: Dict[str, Any],
        project_path: str
    ) -> Dict[str, Any]:
        log_info("ðŸ¤– å¼€å§‹AIåŽå¤„ç†...")
        
        try:
            source_code_map = self._extract_source_code(
                raw_results['issues'],
                project_path
            )
            
            prompt = self._build_analysis_prompt(
                raw_results,
                source_code_map
            )
            
            log_info("ðŸ“¡ è°ƒç”¨æ™ºè°±APIè¿›è¡Œæ™ºèƒ½åˆ†æž...")
            ai_response = await self.llm_client.analyze_with_context(
                prompt=prompt,
                temperature=0.3,
                max_tokens=8000
            )
            
            processed_results = self._parse_ai_response(
                ai_response,
                raw_results
            )
            
            final_results = self._merge_results(
                raw_results,
                processed_results
            )
            
            log_info(f"âœ… AIå¤„ç†å®Œæˆ: {len(raw_results['issues'])} â†’ {len(final_results['issues'])} é—®é¢˜")
            return final_results
            
        except Exception as e:
            log_error(f"AIåŽå¤„ç†å¤±è´¥: {e}")
            log_warning("âš ï¸ é™çº§ä½¿ç”¨åŽŸå§‹æ£€æµ‹ç»“æžœ")
            return raw_results
    
    def _extract_source_code(
        self,
        issues: List[Dict],
        project_path: str
    ) -> Dict[str, str]:
        source_map = {}
        project_root = Path(project_path)
        
        files_to_extract = set()
        for issue in issues:
            for frame in issue.get('stack_trace', []):
                file_path = frame.get('file', '')
                if str(project_root) in file_path:
                    files_to_extract.add(file_path)
            
            if issue.get('file'):
                issue_file = issue['file']
                for f in project_root.rglob('*.cpp'):
                    if f.name == Path(issue_file).name:
                        files_to_extract.add(str(f))
        
        for file_path in files_to_extract:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    source_map[file_path] = f.read()
            except Exception as e:
                log_warning(f"æ— æ³•è¯»å– {file_path}: {e}")
        
        log_info(f"ðŸ“„ æå–äº† {len(source_map)} ä¸ªæºæ–‡ä»¶")
        return source_map
    
    def _build_analysis_prompt(
        self,
        raw_results: Dict,
        source_code_map: Dict[str, str]
    ) -> str:
        simplified_issues = []
        for i, issue in enumerate(raw_results['issues']):
            simplified_issues.append({
                'original_index': i,
                'type': issue['type'],
                'severity': issue['severity'],
                'message': issue['message'],
                'tool': issue['tool'],
                'file': issue.get('file'),
                'line': issue.get('line'),
                'category': issue.get('category'),
                'stack_trace': issue.get('stack_trace', [])[:3]
            })
        
        prompt = f"""# ä»»åŠ¡:æ™ºèƒ½åˆ†æžC++ä»£ç æ¼æ´žæ£€æµ‹ç»“æžœ

## æ£€æµ‹å·¥å…·æŠ¥å‘Š(åŽŸå§‹)
å…±æ£€æµ‹åˆ° {len(simplified_issues)} ä¸ªé—®é¢˜:

```json
{json.dumps(simplified_issues, indent=2, ensure_ascii=False)}
```

## ç›¸å…³æºä»£ç 
"""
        # âš ï¸ è¿™é‡Œæ˜¯é‡ç‚¹ï¼šforå¾ªçŽ¯ä½“å¿…é¡»ç¼©è¿›ï¼
        for file_path, code in list(source_code_map.items())[:10]:
            file_name = Path(file_path).name
            lines = code.split('\n')
            numbered_code = '\n'.join(
                f"{i+1:4d} | {line}"
                for i, line in enumerate(lines[:100])
            )

            prompt += f"""
    æ–‡ä»¶: {file_name}
```cpp
    {numbered_code}
```

"""
    
        prompt += """
ä½ çš„ä»»åŠ¡
è¯·ä½ ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é™æ€åˆ†æžä¸“å®¶,å®Œæˆä»¥ä¸‹å·¥ä½œ:

1. æ™ºèƒ½åŽ»é‡
è¯†åˆ«å®Œå…¨é‡å¤çš„é—®é¢˜(åŒä¸€æ¼æ´žè¢«å¤šä¸ªå·¥å…·/å¤šæ¬¡æ‰§è¡Œæ£€æµ‹åˆ°)

è¯†åˆ«åŒæ ¹é—®é¢˜(åŒä¸€ä¸ªbugçš„ä¸åŒè¡¨çŽ°,å¦‚heap-overflowå¯¼è‡´çš„SEGV)

ä¿ç•™æ¯ç»„é‡å¤ä¸­æœ€è¯¦ç»†çš„é‚£ä¸ª

2. é—®é¢˜åˆ†ç±»
å°†é—®é¢˜åˆ†ç»„ä¸º:

çœŸå®žæ¼æ´ž: ç¡®å®žå­˜åœ¨çš„å®‰å…¨é—®é¢˜

è¯¯æŠ¥: å·¥å…·è¯¯åˆ¤

é‡å¤: ä¸Žå…¶ä»–é—®é¢˜é‡å¤

3. æ ¹å› åˆ†æž
å¯¹æ¯ä¸ªçœŸå®žæ¼æ´ž,æ‰¾å‡º:

æ¼æ´žçš„æ ¹æœ¬åŽŸå› (å“ªè¡Œä»£ç ã€ä»€ä¹ˆé€»è¾‘é”™è¯¯)

CVEç±»åž‹(å¦‚æžœèƒ½è¯†åˆ«,å¦‚CWE-122)

å½±å“èŒƒå›´

4. ä¿®å¤å»ºè®®
ä¸ºæ¯ä¸ªçœŸå®žæ¼æ´žæä¾›:

å…·ä½“çš„ä¿®å¤ä»£ç (diffæ ¼å¼)

å®‰å…¨ç¼–ç å»ºè®®

è¾“å‡ºæ ¼å¼(å¿…é¡»ä¸¥æ ¼JSON)

{{
  "deduplication": {{
    "original_count": 46,
    "unique_count": 5,
    "duplicate_groups": [
      {{
        "representative_index": 0,
        "duplicates": [1, 2, 3],
        "reason": "åŒä¸€ä¸ªå †æº¢å‡ºè¢«ASanã€Valgrindå¤šæ¬¡æ£€æµ‹åˆ°"
      }}
    ]
  }},
  "classification": {{
    "real_vulnerabilities": [
      {{
        "issue_index": 0,
        "type": "heap-buffer-overflow",
        "severity": "critical",
        "file": "vuln_001.cpp",
        "line": 25,
        "root_cause": "reallocå¤±è´¥åŽä»ç„¶memcpy",
        "cve_type": "CWE-122",
        "impact": "å¯é€ æˆè¿œç¨‹ä»£ç æ‰§è¡Œ"
      }}
    ],
    "false_positives": [],
    "duplicates": []
  }},
  "repair_suggestions": [
    {{
      "issue_index": 0,
      "title": "ä¿®å¤å †æº¢å‡ºæ¼æ´ž",
      "description": "æ£€æŸ¥reallocè¿”å›žå€¼",
      "code_diff": "--- a/vuln_001.cpp\\n+++ b/vuln_001.cpp\\n@@ -17,6 +17,9 @@\\n     size_t newCapacity = pool->capacity * 2;\\n     char *newBuffer = (char*)realloc(pool->buffer, newCapacity);\\n-    if (!newBuffer) return -1;\\n+    if (!newBuffer) {{\\n+        return -1;\\n+    }}\\n     pool->buffer = newBuffer;",
      "security_advice": "å§‹ç»ˆæ£€æŸ¥å†…å­˜åˆ†é…æ˜¯å¦æˆåŠŸ,å¤±è´¥æ—¶ä¸è¦ç»§ç»­ä½¿ç”¨åŽŸæŒ‡é’ˆ"
    }}
  ]
}}
é‡è¦:

åªè¾“å‡ºJSON,ä¸è¦ä»»ä½•é¢å¤–æ–‡å­—

æ‰€æœ‰å­—ç¬¦ä¸²ç”¨UTF-8ç¼–ç 

issue_indexæŒ‡å‘åŽŸå§‹issuesæ•°ç»„çš„ç´¢å¼•(original_index)

duplicate_groupsä¸­çš„representative_indexæ˜¯ä¿ç•™çš„ä»£è¡¨issueç´¢å¼•
        """

        return prompt
    

    def _parse_ai_response(
        self,
        ai_response: str,
        raw_results: Dict
    ) -> Dict[str, Any]:
        try:
            json_str = ai_response.strip()
            if '```json' in json_str:
                json_str = json_str.split('```json')[1].split('```')[0]
            elif '```' in json_str:
                json_str = json_str.split('```')[1].split('```')[0]

            parsed = json.loads(json_str)
            return parsed
            
        except json.JSONDecodeError as e:
            log_error(f"AIè¿”å›žçš„JSONæ ¼å¼é”™è¯¯: {e}")
            log_error(f"åŽŸå§‹å“åº”: {ai_response[:500]}")
            return {
                "deduplication": {
                    "original_count": len(raw_results['issues']),
                    "unique_count": len(raw_results['issues']),
                    "duplicate_groups": []
                },
                "classification": {"real_vulnerabilities": []},
                "repair_suggestions": []
            }

    def _merge_results(
        self,
        raw_results: Dict,
        processed: Dict
    ) -> Dict[str, Any]:
        final = raw_results.copy()

        if processed.get('deduplication'):
            dedup = processed['deduplication']
            
            keep_indices = set()
            
            for group in dedup.get('duplicate_groups', []):
                rep_idx = group.get('representative_index')
                if rep_idx is not None:
                    keep_indices.add(rep_idx)
            
            if not keep_indices:
                log_warning("AIæœªè¿”å›žæœ‰æ•ˆåŽ»é‡ç»“æžœ,ä¿ç•™æ‰€æœ‰é—®é¢˜")
                keep_indices = set(range(len(raw_results['issues'])))
            
            original_count = len(raw_results['issues'])
            final['issues'] = [
                issue for i, issue in enumerate(raw_results['issues'])
                if i in keep_indices
            ]
            
            log_info(f"ðŸ—‘ï¸ åŽ»é‡: {original_count} â†’ {len(final['issues'])} é—®é¢˜")
            
            for issue in final['issues']:
                issue['ai_analyzed'] = True
        
        final['ai_classification'] = processed.get('classification', {})
        
        final['repair_suggestions'] = processed.get('repair_suggestions', [])
        
        final['summary']['total_issues'] = len(final['issues'])
        final['summary']['repairs_generated'] = len(final['repair_suggestions'])
        final['summary']['ai_processed'] = True
        
        if processed.get('deduplication'):
            final['summary']['deduplication'] = {
                'original_count': processed['deduplication'].get('original_count', 0),
                'unique_count': len(final['issues']),
                'reduction_rate': f"{(1 - len(final['issues']) / max(processed['deduplication'].get('original_count', 1), 1)) * 100:.1f}%"
            }
        
        return final
_ai_processor_instance = None

def get_ai_postprocessor() -> AIPostProcessor:
        global _ai_processor_instance
        if _ai_processor_instance is None:
            _ai_processor_instance = AIPostProcessor()
        return _ai_processor_instance
