import os
import re
import sys
import traceback
import logging
from typing import Dict, Any, List, Tuple, Optional
from utils.logger import log_info, log_error
from tools.false_positive_filter import FalsePositiveFilter
from tools.priority_scorer import PriorityScorer
from tools.defect_classifier import DefectClassifier
from tools.dynamic_analysis.result_correlator import ResultCorrelator  # â­ æ·»åŠ å¯¼å…¥

# ä¸¥é‡æ€§æ˜ å°„
_SEV_MAP = {
    "error": "high",
    "warning": "medium",
    "style": "low",
    "performance": "medium",
    "portability": "medium",
    "information": "low",
    "debug": "low",
    "critical": "critical",
    "high": "high",
    "medium": "medium",
    "low": "low",
}

def _norm_sev(s: Optional[str]) -> str:
    """å°†å„ç§ severity å­—ç¬¦ä¸²æ ‡å‡†åŒ–ä¸º critical/high/medium/low"""
    if not s:
        return "low"
    s_lower = s.lower()
    return _SEV_MAP.get(s_lower, "low")


class ValidationAgent:
    """
    ç»“æœæ ¡éªŒAgentï¼šè´Ÿè´£è¯¯æŠ¥è¿‡æ»¤ + ä¼˜å…ˆçº§æ’åº + åˆ†ç±»ç»Ÿè®¡ + é™åŠ¨æ€äº¤å‰éªŒè¯
    """

    def __init__(self):
        self.filter = FalsePositiveFilter()
        self.scorer = PriorityScorer()
        self.classifier = DefectClassifier()
        self.result_correlator = ResultCorrelator()  # â­ åŠ¨æ€åˆ†æå…³è”å™¨
        self.validation_rules = self._load_validation_rules()
        self.default_options = {
            "enable_filtering": True,
            "enable_scoring": True,
            "filter_level": "low",
        }
        log_info("ValidationAgent åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒåŠ¨æ€åˆ†æï¼‰")

    def process(
        self,
        issues: List[Dict[str, Any]],
        context: Dict[str, Any],
        options: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """å¤„ç†é™æ€åˆ†æç»“æœï¼ˆåŸæœ‰æ–¹æ³•ï¼‰"""
        raw_issue_count = len(issues) if issues else 0
        log_info(f"[ValidationAgent] process - æ”¶åˆ° {raw_issue_count} ä¸ª issues")

        try:
            print("!!! [DEBUG] Entering ValidationAgent.process try block.", flush=True)

            # åˆå¹¶é…ç½®
            opts: Dict[str, Any] = dict(self.default_options or {})
            opts.update(context.get("options") or {})
            opts.update(options or {})
            log_info(f"[ValidationAgent] process - åˆå¹¶åé€‰é¡¹: {opts}")

            # é˜²å¾¡å¼æ‹·è´
            raw_issues: List[Dict[str, Any]] = [dict(it or {}) for it in (issues or [])]
            before = len(raw_issues)

            if before == 0:
                print("[ValidationAgent] æ”¶åˆ° 0 ä¸ª issues, è·³è¿‡å¤„ç†")
                log_info("[ValidationAgent] process - è¾“å…¥ issues ä¸ºç©º, è·³è¿‡")
                return {
                    "success": True,
                    "issues": [],
                    "statistics": {"before": 0, "after": 0, "filtered": 0},
                    "categories": {},
                }

            # å­—æ®µè§„èŒƒåŒ– / åˆ†ç±»
            log_info("[ValidationAgent] process - å¼€å§‹è§„èŒƒåŒ–å’Œåˆ†ç±»...")
            for i, it in enumerate(raw_issues):
                try:
                    it["file"] = it.get("file") or it.get("path") or "unknown"
                    it["line"] = it.get("line") if isinstance(it.get("line"), int) else 0
                    
                    original_category = it.get("category")
                    calculated_category = self.classifier.classify(it)
                    it["category"] = original_category or calculated_category
                    if not original_category:
                        log_info(f"  - Issue {i}: è‡ªåŠ¨åˆ†ç±»ä¸º '{calculated_category}' (file: {it['file']})")
                    
                    original_severity = it.get("severity")
                    normalized_severity = _norm_sev(original_severity)
                    it["severity"] = normalized_severity
                    if original_severity != normalized_severity:
                        log_info(f"  - Issue {i}: ä¸¥é‡æ€§ä» '{original_severity}' è§„èŒƒåŒ–ä¸º '{normalized_severity}'")
                        
                except Exception as norm_err:
                    log_error(f"[ValidationAgent] process - è§„èŒƒåŒ– Issue {i} æ—¶å‡ºé”™: {norm_err}", exc_info=True)
                    it["category"] = it.get("category", "unknown_norm_error")
                    it["severity"] = it.get("severity", "low")
                    
            log_info("[ValidationAgent] process - è§„èŒƒåŒ–å’Œåˆ†ç±»å®Œæˆ")

            # è¯¯æŠ¥è¿‡æ»¤
            log_info(f"[ValidationAgent] å¼€å§‹è¿‡æ»¤ ({len(raw_issues)} issues)...")
            try:
                filtered = self.filter.apply(raw_issues, context)
                if not isinstance(filtered, list):
                    log_error(f"FalsePositiveFilter.apply() è¿”å›éåˆ—è¡¨: {type(filtered)}")
                    filtered = raw_issues
                log_info(f"[ValidationAgent] è¿‡æ»¤å®Œæˆ: {len(filtered)} issues")
            except Exception as filter_err:
                log_error(f"FalsePositiveFilter å¼‚å¸¸: {filter_err}", exc_info=True)
                filtered = raw_issues

            # å»é‡åˆå¹¶
            log_info(f"[ValidationAgent] process - å¼€å§‹å»é‡åˆå¹¶ (è¾“å…¥ {len(filtered)} ä¸ª issues)...")
            bucket: Dict[Tuple, Dict[str, Any]] = {}
            counts: Dict[Tuple, int] = {}
            sev_rank = {"critical": 3, "high": 2, "medium": 1, "low": 0}
            
            def _key(i: Dict) -> Tuple:
                return (i.get("file", "unknown"), i.get("line", 0), i.get("category", "unknown"))

            for it in filtered:
                k = _key(it)
                if k not in bucket:
                    it["normalized_severity"] = it.get("severity", "low")
                    bucket[k] = it
                    counts[k] = 1
                else:
                    counts[k] += 1
                    cur = bucket[k].get("normalized_severity", "low")
                    inc = it.get("severity", "low")
                    if sev_rank.get(inc, 0) > sev_rank.get(cur, 0):
                        bucket[k]["normalized_severity"] = inc

            deduped: List[Dict[str, Any]] = []
            for k, it in bucket.items():
                it["merged_count"] = counts[k]
                it["severity"] = it.get("normalized_severity", it.get("severity", "low"))
                deduped.append(it)
            log_info(f"[ValidationAgent] process - å»é‡åˆå¹¶å®Œæˆ, è¾“å‡º {len(deduped)} ä¸ª issues")

            # ä¼˜å…ˆçº§è¯„åˆ† & æ’åº
            log_info(f"[ValidationAgent] process - å¼€å§‹ä¼˜å…ˆçº§è¯„åˆ† (è¾“å…¥ {len(deduped)} ä¸ª issues)...")
            ranked_pairs: List[Tuple[float, Dict[str, Any]]] = []
            
            for i, it in enumerate(deduped):
                try:
                    result = self.scorer.score(it, context)
                    
                    if not isinstance(result, tuple) or len(result) != 3:
                        log_error(f"[ValidationAgent] PriorityScorer.score() è¿”å›æ ¼å¼é”™è¯¯ (Issue {i}): {type(result)}, å€¼={result}")
                        score, breakdown, reason = 0.0, {}, "score_format_error"
                    else:
                        score, breakdown, reason = result
                    
                    it["priority_score"] = float(score) if score is not None else 0.0
                    it["score_breakdown"] = breakdown if isinstance(breakdown, dict) else {}
                    it["rank_reason"] = str(reason) if reason else ""
                    ranked_pairs.append((float(it["priority_score"]), it))
                    
                except Exception as score_err:
                    # âœ… æ”¹è¿›: æ›´è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
                    log_error(
                        f"[ValidationAgent] Issue {i} è¯„åˆ†å¤±è´¥: {score_err}\n"
                        f"  æ–‡ä»¶: {it.get('file', 'N/A')}\n"
                        f"  è¡Œå·: {it.get('line', 'N/A')}\n"
                        f"  ç±»å‹: {it.get('category', 'N/A')}",
                        exc_info=True
                    )
                    it["priority_score"] = 0.0
                    it["score_breakdown"] = {}
                    it["rank_reason"] = f"è¯„åˆ†å¼‚å¸¸: {str(score_err)[:50]}"
                    ranked_pairs.append((0.0, it))
            
            log_info("[ValidationAgent] process - ä¼˜å…ˆçº§è¯„åˆ†å®Œæˆ")

            # æ’åº
            log_info("[ValidationAgent] process - å¼€å§‹æ’åº...")
            ranked_pairs.sort(key=lambda p: p[0], reverse=True)
            log_info("[ValidationAgent] process - æ’åºå®Œæˆ")

            # èµ‹äºˆæœ€ç»ˆæ’å
            final_issues: List[Dict[str, Any]] = [p[1] for p in ranked_pairs]
            # âœ… æ–°å¢ï¼šå¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ
            if len(final_issues) == 0:
                log_info("[ValidationAgent] æ‰€æœ‰é—®é¢˜è¢«è¿‡æ»¤ï¼Œè¿”å›ç©ºç»“æœ")
                return {
                    "success": True,
                    "issues": [],
                    "statistics": {"before": before, "after": 0, "filtered": before},
                    "categories": {},
                    "stats": {"before": before, "after": 0, "filtered": before}
                }

            for idx, it in enumerate(final_issues, 1):
                it["rank"] = idx

            # ç»Ÿè®¡ / åˆ†ç±»
            after = len(final_issues)
            stats = {"before": before, "after": after, "filtered": max(0, before - after)}
            categories: Dict[str, int] = {}
            for it in final_issues:
                cat = (it.get("category") or "unknown").lower()
                categories[cat] = categories.get(cat, 0) + 1

            print(f"\n{'='*60}")
            print(f"[ValidationAgent] å¤„ç†å®Œæˆç»Ÿè®¡:")
            print(f"  - è¾“å…¥: {before} issues")
            print(f"  - è¾“å‡º: {after} issues")
            print(f"  - è¿‡æ»¤: {stats['filtered']} issues")
            print(f"{'='*60}\n")

            log_info(f"[ValidationAgent] process - å¤„ç†å®Œæˆ: è¾“å‡º {after} issues, before={before}, filtered={stats['filtered']}")
            print("!!! [DEBUG] ValidationAgent.process try block completed successfully.", flush=True)

            out = {
                "success": True,
                "issues": final_issues,
                "statistics": stats,
                "categories": categories,
            }
            out["stats"] = stats
            return out

        except Exception as e:
            print(f"!!! [FATAL DEBUG] Exception in ValidationAgent: {type(e).__name__}: {e}", file=sys.stderr, flush=True)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush()
            
            log_error(f"[ValidationAgent] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            
            return {
                "success": False,
                "error": str(e),
                "issues": issues,
                "statistics": {"before": raw_issue_count, "after": raw_issue_count, "filtered": 0},
                "categories": {},
            }
    
    # ========== â­ æ–°å¢æ–¹æ³•ï¼šé™åŠ¨æ€äº¤å‰éªŒè¯ â­ ==========
    
    async def validate_static_results(
        self,
        static_issues: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ ¡éªŒé™æ€åˆ†æç»“æœ"""
        try:
            log_info(f"å¼€å§‹æ ¡éªŒé™æ€åˆ†æç»“æœï¼Œå…± {len(static_issues)} ä¸ªé—®é¢˜")
            
            validated_issues = []
            filtered_issues = []
            
            for issue in static_issues:
                if self._validate_issue_structure(issue):
                    normalized = self._normalize_issue(issue)
                    validated_issues.append(normalized)
                else:
                    filtered_issues.append({
                        'issue': issue,
                        'reason': 'é—®é¢˜ç»“æ„ä¸å®Œæ•´'
                    })
            
            log_info(f"æ ¡éªŒå®Œæˆï¼Œæœ‰æ•ˆé—®é¢˜: {len(validated_issues)}, è¿‡æ»¤é—®é¢˜: {len(filtered_issues)}")
            
            return {
                'success': True,
                'validated_issues': validated_issues,
                'filtered_issues': filtered_issues,
                'validation_summary': {
                    'total_input': len(static_issues),
                    'validated': len(validated_issues),
                    'filtered': len(filtered_issues)
                }
            }
            
        except Exception as e:
            log_error(f"é™æ€ç»“æœæ ¡éªŒå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def cross_validate_with_dynamic(
        self,
        static_issues: List[Dict[str, Any]],
        dynamic_issues: List[Dict[str, Any]],
        tolerance: int = 5
    ) -> Dict[str, Any]:
        """é™åŠ¨æ€äº¤å‰éªŒè¯"""
        try:
            log_info("å¼€å§‹é™åŠ¨æ€äº¤å‰éªŒè¯")
            
            # å…ˆæ ¡éªŒé™æ€ç»“æœ
            static_validation = await self.validate_static_results(static_issues)
            
            if not static_validation.get('success'):
                return static_validation
            
            validated_static = static_validation['validated_issues']
            
            # å…³è”é™åŠ¨æ€ç»“æœ
            correlation_result = self.result_correlator.correlate_results(
                validated_static,
                dynamic_issues,
                tolerance
            )
            
            if not correlation_result.get('success'):
                return correlation_result
            
            # æå–å…³è”åçš„é—®é¢˜
            confirmed_issues = correlation_result.get('confirmed_issues', [])
            static_only_issues = correlation_result.get('static_only_issues', [])
            dynamic_only_issues = correlation_result.get('dynamic_only_issues', [])
            
            # åº”ç”¨ç½®ä¿¡åº¦è¿‡æ»¤
            high_confidence = [
                issue for issue in confirmed_issues
                if issue.get('confidence', 0) >= 0.8
            ]
            
            medium_confidence = [
                issue for issue in (confirmed_issues + static_only_issues + dynamic_only_issues)
                if 0.5 <= issue.get('confidence', 0) < 0.8
            ]
            
            low_confidence = [
                issue for issue in (confirmed_issues + static_only_issues + dynamic_only_issues)
                if issue.get('confidence', 0) < 0.5
            ]
            
            # ç”ŸæˆéªŒè¯æŠ¥å‘Š
            validation_report = {
                'high_confidence_issues': high_confidence,
                'medium_confidence_issues': medium_confidence,
                'low_confidence_issues': low_confidence,
                'dynamic_exclusive_issues': dynamic_only_issues,
                'statistics': correlation_result.get('statistics', {}),
                'recommendations': self._generate_recommendations(
                    high_confidence,
                    medium_confidence,
                    low_confidence,
                    dynamic_only_issues
                )
            }
            
            log_info(f"äº¤å‰éªŒè¯å®Œæˆ: é«˜ç½®ä¿¡åº¦ {len(high_confidence)}, ä¸­ç½®ä¿¡åº¦ {len(medium_confidence)}, ä½ç½®ä¿¡åº¦ {len(low_confidence)}")
            
            return {
                'success': True,
                'correlation_result': correlation_result,
                'validation_report': validation_report,
                'total_validated_issues': len(high_confidence) + len(medium_confidence) + len(low_confidence)
            }
            
        except Exception as e:
            log_error(f"äº¤å‰éªŒè¯å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _load_validation_rules(self) -> Dict[str, Any]:
        """åŠ è½½æ ¡éªŒè§„åˆ™"""
        return {
            'min_confidence_threshold': 0.5,
            'require_location': True,
            'severity_mapping': {
                'error': 'high',
                'warning': 'medium',
                'info': 'low',
                'note': 'low'
            }
        }
    
    def _validate_issue_structure(self, issue: Dict[str, Any]) -> bool:
        """éªŒè¯é—®é¢˜ç»“æ„æ˜¯å¦å®Œæ•´"""
        required_fields = ['file', 'line']
        
        for field in required_fields:
            if field not in issue or not issue[field]:
                return False
        
        if not isinstance(issue.get('line'), int) or issue['line'] < 1:
            return False
        
        return True
    
    def _normalize_issue(self, issue: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–é—®é¢˜æ ¼å¼"""
        normalized = issue.copy()
        
        # æ ‡å‡†åŒ–ä¸¥é‡æ€§
        if 'severity' in normalized:
            severity = normalized['severity'].lower()
            normalized['severity'] = self.validation_rules['severity_mapping'].get(
                severity,
                severity
            )
        else:
            normalized['severity'] = 'medium'
        
        # è®¾ç½®é»˜è®¤ç½®ä¿¡åº¦
        if 'confidence' not in normalized:
            normalized['confidence'] = 0.6
        
        # è®¾ç½®é»˜è®¤ä¼˜å…ˆçº§
        if 'priority' not in normalized:
            severity_priority = {
                'critical': 90,
                'high': 70,
                'medium': 50,
                'low': 30
            }
            normalized['priority'] = severity_priority.get(normalized['severity'], 50)
        
        # æ ‡å‡†åŒ–ç±»åˆ«
        if 'category' not in normalized:
            issue_type = normalized.get('type', '').lower()
            if 'leak' in issue_type or 'memory' in issue_type:
                normalized['category'] = 'memory_safety'
            elif 'buffer' in issue_type or 'overflow' in issue_type:
                normalized['category'] = 'memory_safety'
            elif 'null' in issue_type or 'pointer' in issue_type:
                normalized['category'] = 'null_pointer'
            elif 'thread' in issue_type or 'race' in issue_type:
                normalized['category'] = 'concurrency'
            else:
                normalized['category'] = 'general'
        
        return normalized
    
    def _generate_recommendations(
        self,
        high_confidence: List[Dict],
        medium_confidence: List[Dict],
        low_confidence: List[Dict],
        dynamic_only: List[Dict]
    ) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        if high_confidence:
            recommendations.append(
                f"ğŸ”´ å‘ç° {len(high_confidence)} ä¸ªé«˜ç½®ä¿¡åº¦é—®é¢˜ï¼ˆé™åŠ¨æ€ç¡®è®¤ï¼‰ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤"
            )
        
        if medium_confidence:
            recommendations.append(
                f"ğŸŸ¡ å‘ç° {len(medium_confidence)} ä¸ªä¸­ç½®ä¿¡åº¦é—®é¢˜ï¼Œå»ºè®®å®¡æŸ¥åä¿®å¤"
            )
        
        if low_confidence:
            recommendations.append(
                f"âšª å‘ç° {len(low_confidence)} ä¸ªä½ç½®ä¿¡åº¦é—®é¢˜ï¼Œå¯èƒ½å­˜åœ¨è¯¯æŠ¥ï¼Œå»ºè®®äººå·¥éªŒè¯"
            )
        
        if dynamic_only:
            recommendations.append(
                f"ğŸ” åŠ¨æ€åˆ†æç‹¬ç«‹å‘ç° {len(dynamic_only)} ä¸ªé—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–é™æ€åˆ†æè§„åˆ™ä»¥è¦†ç›–è¿™äº›åœºæ™¯"
            )
        
        critical_count = sum(1 for issue in high_confidence if issue.get('severity') == 'critical')
        if critical_count > 0:
            recommendations.append(
                f"âš ï¸ å‘ç° {critical_count} ä¸ªä¸¥é‡é—®é¢˜ï¼Œå»ºè®®ç«‹å³å¤„ç†"
            )
        
        return recommendations
