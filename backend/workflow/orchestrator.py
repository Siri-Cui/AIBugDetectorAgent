# -*- coding: utf-8 -*-
"""å¤šAgentåè°ƒå™¨ï¼ˆå®Œæ•´ç‰ˆï¼šé™æ€+åŠ¨æ€+äº¤å‰éªŒè¯ï¼‰
ä½œç”¨ï¼šåè°ƒæ•´ä¸ªåˆ†ææµç¨‹ï¼Œæ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒæ§åˆ¶å™¨
ä¾èµ–:agentsã€servicesã€database.crudã€utils.logger
è°ƒç”¨å…³ç³»:è¢«analysis APIè°ƒç”¨,åè°ƒå„ä¸ªAgentå·¥ä½œ
"""
import os
from typing import Dict, Any, Optional
import asyncio
import logging
import json
from datetime import datetime, timezone
from os.path import join, isdir

from sqlalchemy.orm import Session
from agents import (
    FileAnalyzerAgent,
    DetectionAgent,
    ContextAnalyzerAgent,
    RepairGeneratorAgent,
)

# å…¼å®¹ä½ æŠŠæ ¡éªŒå·¥å…·æ”¾åœ¨ä¸åŒç›®å½•ï¼šä¼˜å…ˆ agentsï¼Œå¤±è´¥å†å°è¯• tools
try:
    from agents.validation_agent import ValidationAgent
except Exception:
    try:
        from tools.validation_agent import ValidationAgent
    except Exception:
        ValidationAgent = None

from services.analysis_service import AnalysisService
from database.crud import AnalysisCRUD
from utils.logger import log_info, log_error, log_warning
from config import settings

# â­ åŠ¨æ€åˆ†æç›¸å…³å¯¼å…¥
from workflow.dynamic_workflow import DynamicWorkflow
from tools.dynamic_analysis.result_correlator import ResultCorrelator


class Orchestrator:
    """å¤šAgentåè°ƒå™¨ - åˆ†ææµç¨‹çš„å¤§è„‘ï¼ˆæ”¯æŒé™æ€+åŠ¨æ€+äº¤å‰éªŒè¯ï¼‰"""

    def __init__(self, db_session: Session):
        self.db = db_session
        self.analysis_service = AnalysisService(db_session)
        self.analysis_crud = AnalysisCRUD(db_session)

        # åˆå§‹åŒ–æ‰€æœ‰Agent
        self.file_analyzer = FileAnalyzerAgent()
        self.detection_agent = DetectionAgent()
        self.context_analyzer = ContextAnalyzerAgent()
        self.repair_generator = RepairGeneratorAgent()
        self.validation_agent = ValidationAgent() if ValidationAgent else None

        # â­ åŠ¨æ€åˆ†æç»„ä»¶
        self.dynamic_workflow = DynamicWorkflow()
        self.result_correlator = ResultCorrelator()
        
        log_info("Orchestrator åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒåŠ¨æ€åˆ†æï¼‰")

    def _to_dict(self, obj: Any) -> Dict[str, Any]:
        """ç»Ÿä¸€æŠŠ Agent/Pydantic/æ•°æ®ç±» å“åº”è½¬ä¸º dict"""
        if obj is None:
            return {}
        if isinstance(obj, dict):
            return obj
        # Pydantic v2
        if hasattr(obj, "model_dump") and callable(getattr(obj, "model_dump")):
            return obj.model_dump()
        # Pydantic v1
        if hasattr(obj, "dict") and callable(getattr(obj, "dict")):
            return obj.dict()
        # dataclass
        try:
            from dataclasses import is_dataclass, asdict
            if is_dataclass(obj):
                return asdict(obj)
        except Exception:
            pass
        # ä¸€èˆ¬å¯¹è±¡ï¼šå°½é‡å–å±æ€§
        if hasattr(obj, "__dict__"):
            return {
                k: v
                for k, v in obj.__dict__.items()
                if not k.startswith("_") and not callable(v)
            }
        # å…œåº•
        return {
            "success": False,
            "message": f"Unsupported response type: {type(obj).__name__}",
        }

    # ========== é™æ€åˆ†ææµç¨‹ï¼ˆè¿­ä»£6å®Œæ•´ç‰ˆæœ¬ï¼‰==========
    
    async def start_analysis(
        self, project_id: str, analysis_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """å¼€å§‹å®Œæ•´çš„é¡¹ç›®åˆ†ææµç¨‹ï¼ˆé™æ€åˆ†ææ ¸å¿ƒæ–¹æ³•ï¼‰"""
        try:
            log_info(f"å¼€å§‹åˆ†æé¡¹ç›®: {project_id}")

            # 1) è·å–é¡¹ç›®ä¿¡æ¯
            project = await self._get_project_info(project_id)
            if not project:
                return {"success": False, "error": "Project not found"}

            project_path = project["project_path"]

            # 2) åˆ›å»ºæˆ–æ›´æ–°åˆ†æè®°å½•
            if analysis_id:
                analysis_record = self.analysis_crud.get_analysis(analysis_id)
                if not analysis_record:
                    return {"success": False, "error": "Analysis record not found"}
                self.analysis_crud.update_analysis_status(analysis_id, "running")
                analysis_record.id = analysis_id
            else:
                analysis_record = self.analysis_crud.create_analysis(
                    project_id=project_id, analysis_type="static", status="running"
                )

            analysis_result: Dict[str, Any] = {
                "analysis_id": analysis_record.id,
                "project_id": project_id,
                "status": "running",
                "steps": [],
            }

            try:
                # ===== å¤šAgentåä½œæµç¨‹å¼€å§‹ =====

                # æ­¥éª¤1: æ–‡ä»¶ç»“æ„åˆ†æ
                log_info("æ­¥éª¤1: å¼€å§‹æ–‡ä»¶ç»“æ„åˆ†æ")
                file_analysis = await self._step_file_analysis(project)
                file_analysis = self._to_dict(file_analysis)
                analysis_result["steps"].append({
                    "step": "file_analysis",
                    "status": "completed" if file_analysis.get("success") else "failed",
                    "result": file_analysis,
                })
                if not file_analysis.get("success"):
                    raise Exception(f"æ–‡ä»¶åˆ†æå¤±è´¥: {file_analysis.get('message', 'Unknown error')}")

                # æ­¥éª¤2: ä¸Šä¸‹æ–‡åˆ†æ
                log_info("æ­¥éª¤2: å¼€å§‹ä¸Šä¸‹æ–‡åˆ†æ")
                context_analysis = await self._step_context_analysis(file_analysis.get("data", {}))
                context_analysis = self._to_dict(context_analysis)
                analysis_result["steps"].append({
                    "step": "context_analysis",
                    "status": "completed" if context_analysis.get("success") else "failed",
                    "result": context_analysis,
                })
                if not context_analysis.get("success"):
                    log_error("ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
                    context_analysis = {"success": False, "data": {}}

                # æ­¥éª¤3: é™æ€ç¼ºé™·æ£€æµ‹
                log_info("æ­¥éª¤3: å¼€å§‹é™æ€ç¼ºé™·æ£€æµ‹")
                detection_result = await self._step_static_detection(
                    project,
                    file_analysis.get("data", {}),
                    context_analysis.get("data", {}),
                )
                detection_result = self._to_dict(detection_result)
                analysis_result["steps"].append({
                    "step": "static_detection",
                    "status": "completed" if detection_result.get("success") else "failed",
                    "result": detection_result,
                })
                if not detection_result.get("success"):
                    raise Exception(f"é™æ€æ£€æµ‹å¤±è´¥: {detection_result.get('message', 'Unknown error')}")

                # æ­¥éª¤3.5: è¯¯æŠ¥è¿‡æ»¤ + ä¼˜å…ˆçº§æ’åºï¼ˆè¿­ä»£6ï¼‰
                log_info("æ­¥éª¤3.5: è¿›è¡Œè¯¯æŠ¥è¿‡æ»¤ä¸ä¼˜å…ˆçº§æ’åºï¼ˆè¿­ä»£6ï¼‰")
                validated_parsed: Optional[Dict[str, Any]] = None
                
                validation_step = await self._step_validation_and_ranking(
                    detection_result.get("data", {}) or {},
                    context_analysis.get("data", {}) or {},
                    project_path,
                )

                if validation_step.get("success"):
                    validated_data = validation_step.get("data", {})
                    validated_parsed = validated_data.get("parsed_results")
                
                if validated_parsed is not None:
                    dr_data = detection_result.get("data", {})
                    dr_data["parsed_results"] = validated_parsed
                    detection_result["data"] = dr_data
                    detection_result["_validated"] = True  # â­ æ ‡è®°å·²éªŒè¯
                    
                    issues_count = len(validated_parsed.get('issues', []))
                    log_info(f"âœ… å·²æ›´æ–°detection_result: {issues_count} issues")
                else:
                    log_error("validated_parsed ä¸º None,æœªæ›´æ–° detection_result")

                analysis_result["steps"].append({
                    "step": "validation_and_ranking",
                    "status": "completed" if validation_step.get("success") else "skipped",
                    "result": validation_step,
                })

                # æ­¥éª¤4: AIä¿®å¤å»ºè®®ç”Ÿæˆ
                log_info("æ­¥éª¤4: å¼€å§‹ç”ŸæˆAIä¿®å¤å»ºè®®ï¼ˆåŸºäºçœŸå®ä»£ç ï¼‰")
                repair_suggestions = await self._step_repair_generation(
                    detection_result.get("data", {}),
                    file_analysis.get("data", {}),
                    context_analysis.get("data", {}),
                    project_path,
                )
                repair_suggestions = self._to_dict(repair_suggestions)
                analysis_result["steps"].append({
                    "step": "repair_generation",
                    "status": "completed" if repair_suggestions.get("success") else "skipped",
                    "result": repair_suggestions,
                })

                # ===== å¤šAgentåä½œæµç¨‹ç»“æŸ =====

                # 5) ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
                final_report = await self._generate_final_report(
                    file_analysis.get("data", {}),
                    detection_result.get("data", {}),
                    context_analysis.get("data", {}),
                    repair_suggestions.get("data", {}),
                )

                # 6) ä¿å­˜ç»“æœ
                await self._save_analysis_results(analysis_record.id, final_report)

                # 7) æ›´æ–°åˆ†æçŠ¶æ€
                self.analysis_crud.update_analysis_status(analysis_record.id, "completed")

                analysis_result.update({
                    "success": True,
                    "status": "completed",
                    "final_report": final_report,
                    "message": f'åˆ†æå®Œæˆï¼Œå‘ç° {final_report["summary"]["total_issues"]} ä¸ªé—®é¢˜',
                })

                log_info(f"é¡¹ç›® {project_id} åˆ†æå®Œæˆ")
                return analysis_result

            except Exception as step_error:
                self.analysis_crud.update_analysis_status(
                    analysis_record.id, "failed", error_message=str(step_error)
                )
                raise step_error

        except Exception as e:
            log_error(f"åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e), "project_id": project_id}

    # ========== åŠ¨æ€åˆ†ææµç¨‹ï¼ˆæ–°å¢ï¼‰==========
    
    async def start_dynamic_analysis(
        self,
        project_id: str,
        analysis_id: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å¯åŠ¨åŠ¨æ€åˆ†æå·¥ä½œæµ"""
        try:
            log_info(f"[Orchestrator] å¼€å§‹åŠ¨æ€åˆ†æ: project_id={project_id}, analysis_id={analysis_id}")
            
            # 1. æ›´æ–°åˆ†æçŠ¶æ€
            self.analysis_crud.update_analysis_status(
                analysis_id,
                status="running",
                error_message="åŠ¨æ€åˆ†æè¿›è¡Œä¸­..."
            )
            
            # 2. è·å–é¡¹ç›®è·¯å¾„
            analysis_record = self.analysis_crud.get_analysis(analysis_id)
            if not analysis_record:
                raise ValueError(f"åˆ†æè®°å½•ä¸å­˜åœ¨: {analysis_id}")
            
            root_dir = join(settings.UPLOAD_DIR, project_id)
            extracted_dir = join(root_dir, "extracted")
            project_path = extracted_dir if isdir(extracted_dir) else root_dir
            
            # 3. æ‰§è¡ŒåŠ¨æ€åˆ†æå·¥ä½œæµ
            log_info("[Orchestrator] è°ƒç”¨ DynamicWorkflow...")
            dynamic_result = await self.dynamic_workflow.run_dynamic_analysis_workflow(
                project_id=project_id,
                project_path=project_path,
                config=config,
                static_results=None
            )
            
            if not dynamic_result.get("success"):
                log_error(f"[Orchestrator] åŠ¨æ€åˆ†æå¤±è´¥: {dynamic_result.get('error')}")
                self.analysis_crud.update_analysis_status(
                    analysis_id,
                    status="failed",
                    error_message=f"åŠ¨æ€åˆ†æå¤±è´¥: {dynamic_result.get('error')}"
                )
                return dynamic_result
            
            # 4. ä¿å­˜ç»“æœ
            log_info("[Orchestrator] ä¿å­˜åŠ¨æ€åˆ†æç»“æœ...")
            result_file_path = os.path.join(
                config.get("output_dir", "/tmp/dynamic_analysis"),
                f"dynamic_result_{analysis_id}.json"
            )
            
            os.makedirs(os.path.dirname(result_file_path), exist_ok=True)
            with open(result_file_path, 'w', encoding='utf-8') as f:
                json.dump(dynamic_result, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°æ•°æ®åº“
            self.analysis_crud.update_analysis_status(
                analysis_id,
                status="completed"
            )
            
            log_info("[Orchestrator] åŠ¨æ€åˆ†æå®Œæˆ")
            
            return {
                "success": True,
                "analysis_id": analysis_id,
                "result": dynamic_result,
                "message": "åŠ¨æ€åˆ†æå®Œæˆ"
            }
            
        except Exception as e:
            log_error(f"[Orchestrator] åŠ¨æ€åˆ†æå¼‚å¸¸: {e}", exc_info=True)
            self.analysis_crud.update_analysis_status(
                analysis_id,
                status="failed",
                error_message=f"åŠ¨æ€åˆ†æå¼‚å¸¸: {str(e)}"
            )
            return {
                "success": False,
                "error": str(e),
                "analysis_id": analysis_id
            }
    
    async def start_full_analysis_with_dynamic(
        self,
        project_id: str,
        analysis_id: str,
        enable_dynamic: bool = True,
        dynamic_config: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """å®Œæ•´åˆ†ææµç¨‹ï¼ˆé™æ€ + åŠ¨æ€ + äº¤å‰éªŒè¯ï¼‰- æœ€ç»ˆä¼˜åŒ–ç‰ˆ"""
        import time
        
        timing = {}  # è®°å½•å„é˜¶æ®µè€—æ—¶
        
        try:
            log_info(f"[Orchestrator] ğŸš€ å¼€å§‹å®Œæ•´åˆ†ææµç¨‹ï¼ˆenable_dynamic={enable_dynamic}ï¼‰")
            total_start = time.time()
            
            # ========== é˜¶æ®µ1: é™æ€åˆ†æ ==========
            phase_start = time.time()
            log_info("[Orchestrator] ğŸ“Š é˜¶æ®µ1/4: é™æ€åˆ†æ...")
            
            static_result = await self.start_analysis(project_id, analysis_id)
            
            if not static_result.get("success"):
                return static_result
            
            timing['static'] = round(time.time() - phase_start, 2)
            log_info(f"[Orchestrator] âœ… é™æ€åˆ†æå®Œæˆï¼Œè€—æ—¶: {timing['static']}s")
            
            # æå–é™æ€åˆ†æç»“æœ
            final_report = static_result.get("final_report", {})
            static_issues = final_report.get("issues", [])
            
            # åˆå§‹åŒ– summaryï¼ˆé¿å…åç»­ KeyErrorï¼‰
            if "summary" not in final_report:
                final_report["summary"] = {}
            summary = final_report["summary"]
            
            # å¦‚æœä¸å¯ç”¨åŠ¨æ€åˆ†æï¼Œè¡¥å……é»˜è®¤å€¼åè¿”å›
            if not enable_dynamic:
                log_info("[Orchestrator] â­ï¸  åŠ¨æ€åˆ†ææœªå¯ç”¨ï¼Œè¡¥å……é»˜è®¤ä¿¡æ¯åè¿”å›")
                summary["dynamic_analysis"] = {"executed": False}
                summary["cross_validation"] = {
                    "high_confidence": 0,
                    "medium_confidence": len(static_issues),
                    "low_confidence": 0,
                    "total_validated": len(static_issues)
                }
                summary["performance"] = {
                    "total_time": timing['static'],
                    "static_time": timing['static'],
                    "dynamic_time": 0,
                    "validation_time": 0,
                    "ai_repair_time": 0
                }
                
                # ä¿å­˜ç»“æœ
                await self.analysis_service.save_analysis_report(analysis_id, final_report)
                
                return {
                    "success": True,
                    "message": "é™æ€åˆ†æå®Œæˆï¼ˆæœªå¯ç”¨åŠ¨æ€åˆ†æï¼‰",
                    "analysis_id": analysis_id,
                    "result": final_report
                }
            
            # ========== é˜¶æ®µ2: åŠ¨æ€åˆ†æ ==========
            phase_start = time.time()
            log_info("[Orchestrator] ğŸ” é˜¶æ®µ2/4: åŠ¨æ€åˆ†æ...")
            
            dynamic_config = dynamic_config or {}
            dynamic_result = await self.start_dynamic_analysis(
                project_id,
                analysis_id,
                dynamic_config
            )
            
            timing['dynamic'] = round(time.time() - phase_start, 2)
            
            # æå–åŠ¨æ€åˆ†ææ•°æ®
            dynamic_executed = dynamic_result.get("success", False)
            dynamic_data = dynamic_result.get("result", {}) if dynamic_executed else {}
            dynamic_issues = dynamic_data.get("dynamic_issues", [])
            
            # âœ… æ·»åŠ è°ƒè¯•æ—¥å¿—
            log_info(f"ğŸ” è°ƒè¯•ï¼šdynamic_data åŒ…å«çš„é”®: {list(dynamic_data.keys())}")
            if "dynamic_execution" in dynamic_data:
                log_info(f"   dynamic_execution å†…å®¹: {dynamic_data['dynamic_execution']}")
            else:
                log_warning("   âš ï¸  ç¼ºå°‘ dynamic_execution å­—æ®µï¼")

            # æ„å»ºåŠ¨æ€åˆ†æç»Ÿè®¡
            dynamic_stats = self._build_dynamic_stats(dynamic_data, dynamic_issues, timing['dynamic'])
            
            log_info(f"[Orchestrator] âœ… åŠ¨æ€åˆ†æå®Œæˆï¼Œè€—æ—¶: {timing['dynamic']}s, "
                    f"å‘ç° {len(dynamic_issues)} ä¸ªé—®é¢˜")
            if dynamic_executed:
                log_info("="*70)
                log_info("ğŸ” åŠ¨æ€åˆ†æç»“æœè¯Šæ–­")
                log_info("="*70)
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                log_info(f"æ‰§è¡ŒçŠ¶æ€:")
                log_info(f"   Valgrind: {'âœ…' if dynamic_stats.get('valgrind_executed') else 'âŒ'}")
                log_info(f"   ASan: {'âœ…' if dynamic_stats.get('asan_executed') else 'âŒ'}")
                log_info(f"   UBSan: {'âœ…' if dynamic_stats.get('ubsan_executed') else 'âŒ'}")
                
                log_info(f"\né—®é¢˜ç»Ÿè®¡:")
                log_info(f"   Valgrind é—®é¢˜æ•°: {dynamic_stats.get('valgrind_issues', 0)}")
                log_info(f"   ASan é—®é¢˜æ•°: {dynamic_stats.get('asan_issues', 0)}")
                log_info(f"   åŠ¨æ€æ€»é—®é¢˜æ•°: {len(dynamic_issues)}")
                
                if dynamic_issues:
                    log_info(f"\nå‰5ä¸ªåŠ¨æ€é—®é¢˜è¯¦æƒ…:")
                    for i, issue in enumerate(dynamic_issues[:5], 1):
                        log_info(f"   {i}. [{issue.get('severity', '?')}] {issue.get('type')}")
                        log_info(f"      å­ç±»å‹: {issue.get('subtype', 'N/A')}")
                        log_info(f"      å·¥å…·: {issue.get('tool', 'N/A')}")
                        log_info(f"      ä½ç½®: {issue.get('location', 'N/A')}")
                else:
                    log_warning("\nâš ï¸  æœªå‘ç°ä»»ä½•åŠ¨æ€é—®é¢˜ï¼")
                    log_warning("è¯·æ£€æŸ¥:")
                    log_warning("   1. ç¼–è¯‘æ—¥å¿—ä¸­æ˜¯å¦æœ‰ -fsanitize æ ‡å¿—")
                    log_warning("   2. å¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦é“¾æ¥äº† libasan/libubsan")
                    log_warning("   3. ç¨‹åºæ˜¯å¦çœŸæ­£è¿è¡Œï¼ˆæŸ¥çœ‹æ‰§è¡Œå™¨æ—¥å¿—ï¼‰")
                    log_warning("   4. è§£æå‡½æ•°æ˜¯å¦æ­£ç¡®å·¥ä½œ")
                
                log_info("="*70)            
            # ========== é˜¶æ®µ3: äº¤å‰éªŒè¯ ==========
            phase_start = time.time()
            log_info("[Orchestrator] ğŸ”— é˜¶æ®µ3/4: äº¤å‰éªŒè¯...")

            cross_validation_stats = None
            validated_issues = []  # â­ åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨

            if dynamic_executed and dynamic_issues and self.validation_agent:
                try:
                    cross_validation_result = await self.validation_agent.cross_validate_with_dynamic(
                        static_issues,
                        dynamic_issues,
                        tolerance=dynamic_config.get("line_tolerance", 5)
                    )
                    
                    if cross_validation_result.get("success"):
                        validation_report = cross_validation_result.get("validation_report", {})
                        
                        # âœ… æå–æ‰€æœ‰ç±»å‹çš„é—®é¢˜
                        high_confidence = validation_report.get("high_confidence_issues", [])
                        medium_confidence = validation_report.get("medium_confidence_issues", [])
                        low_confidence = validation_report.get("low_confidence_issues", [])
                        dynamic_only = validation_report.get("dynamic_exclusive_issues", [])  # â­ å…³é”®
                        
                        # âœ… åˆå¹¶æ‰€æœ‰é—®é¢˜ï¼ˆåŒ…æ‹¬ä»…åŠ¨æ€å‘ç°çš„ï¼‰
                        validated_issues = high_confidence + medium_confidence + low_confidence + dynamic_only
                        
                        # ç»Ÿè®¡å„ç½®ä¿¡åº¦çº§åˆ«
                        cross_validation_stats = {
                            "high_confidence": len(high_confidence),
                            "medium_confidence": len(medium_confidence),
                            "low_confidence": len(low_confidence),
                            "dynamic_only": len(dynamic_only),  # â­ æ–°å¢
                            "total_validated": len(validated_issues)
                        }
                        
                        log_info(f"[Orchestrator] âœ… äº¤å‰éªŒè¯å®Œæˆ: "
                                f"é«˜={len(high_confidence)}, "
                                f"ä¸­={len(medium_confidence)}, "
                                f"ä½={len(low_confidence)}, "
                                f"ä»…åŠ¨æ€={len(dynamic_only)}")  # â­ æ˜¾ç¤ºä»…åŠ¨æ€
                    else:
                        log_warning("[Orchestrator] âš ï¸  äº¤å‰éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åŠ¨æ€ç»“æœ")
                        validated_issues = dynamic_issues  # â­ å¤±è´¥æ—¶ä¿ç•™åŠ¨æ€ç»“æœ
                        
                except Exception as e:
                    log_error(f"[Orchestrator] âŒ äº¤å‰éªŒè¯å¼‚å¸¸: {e}", exc_info=True)
                    validated_issues = dynamic_issues  # â­ å¼‚å¸¸æ—¶ä¿ç•™åŠ¨æ€ç»“æœ

            elif dynamic_executed and dynamic_issues:
                # æœ‰åŠ¨æ€ç»“æœä½†æ— éªŒè¯å™¨
                log_warning("[Orchestrator] âš ï¸  ValidationAgent ä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨åŠ¨æ€ç»“æœ")
                validated_issues = dynamic_issues  # â­ æ— éªŒè¯å™¨æ—¶ä¿ç•™åŠ¨æ€ç»“æœ

            else:
                # æ— åŠ¨æ€åˆ†ææˆ–åŠ¨æ€ç»“æœä¸ºç©º
                log_info("[Orchestrator] â„¹ï¸  æ— åŠ¨æ€ç»“æœï¼Œä½¿ç”¨é™æ€ç»“æœ")
                validated_issues = static_issues

            timing['validation'] = round(time.time() - phase_start, 2)

            # é»˜è®¤äº¤å‰éªŒè¯ç»Ÿè®¡ï¼ˆå¦‚æœæ²¡æœ‰æ‰§è¡ŒéªŒè¯ï¼‰
            if cross_validation_stats is None:
                cross_validation_stats = {
                    "high_confidence": 0,
                    "medium_confidence": 0,
                    "low_confidence": 0,
                    "dynamic_only": len(validated_issues) if dynamic_executed else 0,
                    "total_validated": len(validated_issues)
                }

            
            # ========== é˜¶æ®µ4: æ•´åˆæœ€ç»ˆæŠ¥å‘Š ==========
            log_info("[Orchestrator] ğŸ“‹ é˜¶æ®µ4/4: æ•´åˆæœ€ç»ˆæŠ¥å‘Š...")
            
            # æ›´æ–° issuesï¼ˆä½¿ç”¨äº¤å‰éªŒè¯åçš„ï¼‰
            final_report["issues"] = validated_issues
            
            # æ›´æ–°å·¥å…·åˆ—è¡¨
            tools = set(summary.get("analysis_tools", []))
            tools.update(dynamic_stats.get("tools", []))
            summary["analysis_tools"] = sorted(list(tools))
            
            # æ·»åŠ åŠ¨æ€åˆ†æä¿¡æ¯
            summary["dynamic_analysis"] = dynamic_stats
            
            # æ·»åŠ äº¤å‰éªŒè¯ç»Ÿè®¡
            summary["cross_validation"] = cross_validation_stats
            
            # æ·»åŠ æ€§èƒ½ç»Ÿè®¡
            timing['total'] = round(time.time() - total_start, 2)
            summary["performance"] = {
                "total_time": timing['total'],
                "static_time": timing.get('static', 0),
                "dynamic_time": timing.get('dynamic', 0),
                "validation_time": timing.get('validation', 0),
                "ai_repair_time": 0  # å¦‚æœæœ‰AIä¿®å¤æ—¶é—´ï¼Œä» static_result æå–
            }
            
            # æ›´æ–°æ€»é—®é¢˜æ•°
            summary["total_issues"] = len(validated_issues)
            
            # é‡æ–°è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
            severity_dist = {}
            for issue in validated_issues:
                sev = (issue.get("severity") or "unknown").lower()
                severity_dist[sev] = severity_dist.get(sev, 0) + 1
            summary["severity_distribution"] = severity_dist
            
            # ========== ä¿å­˜ç»“æœ ==========
            log_info("[Orchestrator] ğŸ’¾ ä¿å­˜ç»“æœåˆ°æ•°æ®åº“å’Œæ–‡ä»¶...")
            await self.analysis_service.save_analysis_report(analysis_id, final_report)
            
            # æ›´æ–°æ•°æ®åº“ï¼ˆç›´æ¥æ“ä½œæ•°æ®åº“å¯¹è±¡ï¼‰
            analysis_record = self.analysis_crud.get_analysis(analysis_id)
            if analysis_record:
                analysis_record.status = "completed"
                analysis_record.end_time = datetime.now(timezone.utc)
                analysis_record.duration = timing['total']
                analysis_record.total_defects = summary["total_issues"]
                analysis_record.high_defects = severity_dist.get("high", 0)
                analysis_record.medium_defects = severity_dist.get("medium", 0)
                analysis_record.low_defects = severity_dist.get("low", 0)
                self.db.commit()
                log_info("[Orchestrator] âœ… æ•°æ®åº“è®°å½•å·²æ›´æ–°")
                        
            log_info(f"[Orchestrator] ğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼æ€»è€—æ—¶: {timing['total']}s")
            
            return {
                "success": True,
                "message": f"åˆ†æå®Œæˆï¼Œå‘ç° {summary['total_issues']} ä¸ªé—®é¢˜ "
                        f"(åŠ¨æ€ç¡®è®¤: {cross_validation_stats['high_confidence']})",
                "analysis_id": analysis_id,
                "result": final_report
            }
            
        except Exception as e:
            log_error(f"[Orchestrator] âŒ å®Œæ•´åˆ†ææµç¨‹å¼‚å¸¸: {e}", exc_info=True)
            self.analysis_crud.update_analysis_status(
                analysis_id,
                status="failed",
                error_message=str(e)
            )
            return {
                "success": False,
                "error": str(e),
                "analysis_id": analysis_id
            }

    # ========== æ–°å¢è¾…åŠ©æ–¹æ³• ==========

    def _build_dynamic_stats(
        self,
        dynamic_data: Dict[str, Any],
        dynamic_issues: list,
        execution_time: float
    ) -> Dict[str, Any]:
        """æ„å»ºåŠ¨æ€åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        
        # âœ… ä¿®å¤ï¼šä»æ­£ç¡®çš„åµŒå¥—è·¯å¾„è¯»å–æ‰§è¡ŒçŠ¶æ€
        dynamic_execution = dynamic_data.get("dynamic_execution", {})
        
        stats = {
            "executed": bool(dynamic_data),
            "valgrind_executed": dynamic_execution.get("valgrind_executed", False),  # âœ… ä¿®å¤
            "asan_executed": dynamic_execution.get("asan_executed", False),          # âœ… ä¿®å¤
            "ubsan_executed": dynamic_execution.get("ubsan_executed", False),        # âœ… æ–°å¢
            "valgrind_issues": dynamic_execution.get("valgrind_issues", 0),          # âœ… ç›´æ¥è¯»å–
            "asan_issues": dynamic_execution.get("asan_issues", 0),                  # âœ… ç›´æ¥è¯»å–
            "ubsan_issues": dynamic_execution.get("ubsan_issues", 0),                # âœ… æ–°å¢
            "execution_time": execution_time,
            "tools": dynamic_execution.get("tools_run", [])                          # âœ… ç›´æ¥è¯»å–å·¥å…·åˆ—è¡¨
        }
        
        # âš ï¸ å¦‚æœ dynamic_execution ä¸ºç©ºï¼Œå°è¯•ä» issues æ¨æ–­ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        if not dynamic_execution:
            log_warning("âš ï¸  dynamic_execution å­—æ®µä¸ºç©ºï¼Œä» issues æ¨æ–­æ‰§è¡ŒçŠ¶æ€")
            stats["valgrind_issues"] = 0
            stats["asan_issues"] = 0
            stats["ubsan_issues"] = 0
            
            for issue in dynamic_issues:
                tool = issue.get('source_tool', '').lower()
                if 'valgrind' in tool:
                    stats["valgrind_issues"] += 1
                    stats["valgrind_executed"] = True
                if 'asan' in tool or 'address' in tool:
                    stats["asan_issues"] += 1
                    stats["asan_executed"] = True
                if 'ubsan' in tool or 'undefined' in tool:
                    stats["ubsan_issues"] += 1
                    stats["ubsan_executed"] = True
            
            # é‡å»ºå·¥å…·åˆ—è¡¨
            stats["tools"] = []
            if stats["valgrind_executed"]:
                stats["tools"].append("valgrind_memcheck")
            if stats["asan_executed"]:
                stats["tools"].append("address_sanitizer")
            if stats["ubsan_executed"]:
                stats["tools"].append("undefined_sanitizer")
        
        return stats

    # ========== è¾…åŠ©æ–¹æ³•ï¼ˆç§æœ‰ï¼‰==========
    
    async def _run_validation_in_thread(self, issues: list, context: dict) -> dict:
        """å°†åŒæ­¥çš„ ValidationAgent.process æ”¾å…¥çº¿ç¨‹æ± æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯"""
        if not self.validation_agent:
            return {"success": False, "message": "ValidationAgent not available"}
        return await asyncio.to_thread(self.validation_agent.process, issues, context)

    async def _step_validation_and_ranking(
        self,
        detection_data: Dict[str, Any],
        context_data: Dict[str, Any],
        project_path: str,
    ) -> Dict[str, Any]:
        """æ­¥éª¤3.5ï¼šè¯¯æŠ¥è¿‡æ»¤ + ä¼˜å…ˆçº§æ’åºï¼ˆè¿­ä»£6æ ¸å¿ƒï¼‰"""
        try:
            context_opts = context_data.get("options") or {}
            enable_validation = context_opts.get(
                "enable_validation", getattr(settings, "ENABLE_VALIDATION", True)
            )
            if not enable_validation:
                logging.info("Validation disabled. è·³è¿‡è¿­ä»£6")
                return {"success": False, "message": "Validation disabled"}

            parsed = detection_data.get("parsed_results", {}) or {}
            issues = parsed.get("issues", []) or []
            if not issues:
                return {"success": False, "message": "No issues to validate"}

            before_cnt = len(issues)
            log_info(f"ğŸ” å¼€å§‹éªŒè¯ï¼š{before_cnt} ä¸ªåŸå§‹issues")

            v_context = {
                "project_path": project_path,
                "project_features": (context_data.get("project_features") or []),
                "options": context_opts,
            }
            vout = await self._run_validation_in_thread(issues, v_context)
            if not vout or not vout.get("success"):
                log_error("ValidationAgent æœªè¿”å›æˆåŠŸç»“æœï¼Œä¿ç•™åŸå§‹ issues")
                return {
                    "success": False,
                    "message": vout.get("message", "validation failed"),
                }

            filtered = vout.get("issues", []) or []
            after_cnt = len(filtered)
            
            log_info(f"âœ… éªŒè¯å®Œæˆï¼šbefore={before_cnt} â†’ after={after_cnt} (è¿‡æ»¤{before_cnt-after_cnt}ä¸ª)")

            return {
                "success": True,
                "data": {
                    "validated_before": before_cnt,
                    "validated_after": after_cnt,
                    "validated_filtered": max(0, before_cnt - after_cnt),
                    "parsed_results": {
                        "issues": filtered,
                        "statistics": {
                            "validated_before": before_cnt,
                            "validated_after": after_cnt,
                            "validated_filtered": max(0, before_cnt - after_cnt),
                        },
                        "categories": vout.get("categories", {}),
                    },
                },
            }
        
        except Exception as exc:
            log_error(f"Validation step failed: {exc}", exc_info=True)
            return {"success": False, "message": str(exc)}

    async def _get_project_info(self, project_id: str) -> Optional[Dict[str, Any]]:
        """è·å–é¡¹ç›®ä¿¡æ¯"""
        project_path = os.path.join(settings.UPLOAD_DIR, project_id, "extracted")
        return {
            "id": project_id,
            "project_path": project_path,
            "name": f"Project_{project_id}",
        }

    async def _step_file_analysis(self, project: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥éª¤1: æ–‡ä»¶ç»“æ„åˆ†æ"""
        task_data = {
            "project_path": project["project_path"],
            "project_id": project["id"],
        }
        result = await self.file_analyzer.process(task_data)
        return self._to_dict(result)

    async def _step_context_analysis(self, file_analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥éª¤2: ä¸Šä¸‹æ–‡åˆ†æ"""
        task_data = {"file_analysis": file_analysis_data}
        result = await self.context_analyzer.process(task_data)
        return self._to_dict(result)

    async def _step_static_detection(
        self,
        project: Dict[str, Any],
        file_analysis_data: Dict[str, Any],
        context_data: Dict[str, Any],
    ) -> Dict[str, Any]:
        """æ­¥éª¤3: é™æ€ç¼ºé™·æ£€æµ‹"""
        analysis_config = {
            "enable_cppcheck": True,
            "source_files": file_analysis_data.get("source_files", []),
            "context": context_data,
        }
        task_data = {
            "project_path": project["project_path"],
            "analysis_config": analysis_config,
        }
        result = await self.detection_agent.process(task_data)
        return self._to_dict(result)

    async def _step_repair_generation(
        self,
        detection_results: Dict[str, Any],
        file_analysis_data: Dict[str, Any],
        context_data: Dict[str, Any],
        project_path: str,
    ) -> Dict[str, Any]:
        """æ­¥éª¤4: AIä¿®å¤å»ºè®®ç”Ÿæˆï¼ˆè¿­ä»£4å¢å¼ºï¼šåŸºäºçœŸå®ä»£ç ï¼‰"""
        task_data = {
            "detection_results": detection_results,
            "file_analysis": file_analysis_data,
            "context": context_data,
            "project_path": project_path,
        }
        result = await self.repair_generator.process(task_data)
        return self._to_dict(result)

    async def _generate_final_report(
        self,
        file_analysis: Dict[str, Any],
        detection_results: Dict[str, Any],
        context_analysis: Dict[str, Any],
        repair_suggestions: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        parsed_results = detection_results.get("parsed_results", {}) or {}
        issues_after = parsed_results.get("issues", []) or []
        
        # éªŒè¯
        if not detection_results.get("_validated"):
            log_error("âš ï¸ è­¦å‘Šï¼šdetection_results æœªç»è¿‡ validationï¼Œæ•°æ®å¯èƒ½ä¸å®Œæ•´ï¼")
        
        # âœ… ä¿®å¤ï¼šæ·»åŠ ç©ºåˆ—è¡¨æ£€æŸ¥
        if issues_after:
            if not issues_after[0].get("priority_score"):
                log_error("âš ï¸ è­¦å‘Šï¼šissues ç¼ºå°‘ priority_score å­—æ®µï¼Œæ ¡éªŒå¯èƒ½å¤±è´¥ï¼")
            else:
                log_info(f"âœ… éªŒè¯é€šè¿‡ï¼šç¬¬ä¸€ä¸ª issue æœ‰ priority_score = {issues_after[0].get('priority_score')}")
        else:
            log_info("â„¹ï¸  æœªå‘ç°é—®é¢˜ï¼Œè·³è¿‡ priority_score éªŒè¯")
        
        total_issues = len(issues_after)
        
        # é‡ç®— severity_distribution
        sev_count = {}
        for it in issues_after:
            sev = (it.get("severity") or "unknown").lower()
            sev_count[sev] = sev_count.get(sev, 0) + 1
        severity_dist = sev_count or parsed_results.get("statistics", {}).get("severity_distribution", {})
        
        # å–å‡ºæ ¡éªŒç»Ÿè®¡
        stats = parsed_results.get("statistics", {}) or {}
        validated_before = stats.get("validated_before")
        validated_after = stats.get("validated_after")
        validated_filtered = stats.get("validated_filtered")

        repairs = repair_suggestions.get("repair_suggestions", [])

        report = {
            "summary": {
                "total_issues": total_issues,
                "files_analyzed": len(file_analysis.get("source_files", [])),
                "severity_distribution": severity_dist,
                "analysis_tools": list(detection_results.get("tool_results", {}).keys()),
                "repairs_generated": len(repairs),
                "repairs_with_real_code": len([
                    r for r in repairs
                    if r.get("type") == "llm_generated_with_context"
                ]),
                **(
                    {
                        "validated_before": validated_before,
                        "validated_after": validated_after,
                        "validated_filtered": validated_filtered,
                    }
                    if validated_before is not None
                    else {}
                ),
            },
            "file_analysis": {
                "project_structure": file_analysis.get("project_structure", {}),
                "complexity_metrics": file_analysis.get("complexity_metrics", {}),
            },
            "context_analysis": {
                "macros": context_analysis.get("macros", {}),
                "platform_info": context_analysis.get("platform_info", {}),
                "compiler_info": context_analysis.get("compiler_info", {}),
            },
            "issues": issues_after,
            "recommendations": detection_results.get("recommendations", []),
            "repair_suggestions": repairs,
        }
        return report

    async def _save_analysis_results(self, analysis_id: str, report: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            if hasattr(self.analysis_crud, "save_analysis_results"):
                try:
                    self.analysis_crud.save_analysis_results(analysis_id, report)
                except Exception as e_db:
                    log_error(f"ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“å¤±è´¥ï¼ˆå°†ç»§ç»­å†™æ–‡ä»¶ï¼‰: {str(e_db)}")

            await self.analysis_service.save_analysis_report(analysis_id, report)
            log_info(f"åˆ†æç»“æœå·²ä¿å­˜: {analysis_id}")
        except Exception as e:
            log_error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}")
            raise
