# -*- coding: utf-8 -*-
"""ç»“æœè§£æå™¨
ä½œç”¨ï¼šç»Ÿä¸€å¤„ç†ä¸åŒå·¥å…·çš„è¾“å‡ºï¼Œæ ‡å‡†åŒ–ç»“æœæ ¼å¼ï¼Œè¿‡æ»¤å™ªéŸ³
ä¾èµ–ï¼šutils.logger
è°ƒç”¨å…³ç³»ï¼šè¢«DetectionAgentè°ƒç”¨
"""
import re
from typing import Dict, List, Any, Optional
from utils.logger import log_info, log_error


class ResultParser:
    def __init__(self):
        self.severity_map = {
            'error': 'high',
            'warning': 'medium',
            'info': 'low',
            'style': 'low',
            'performance': 'medium',
            'portability': 'low',
            'information': 'info',
            'high': 'high',
            'medium': 'medium',
            'low': 'low',
            'critical': 'high',
            'note': 'info'
        }
        
        # ğŸ†• æ–°å¢ï¼šå®šä¹‰éœ€è¦è¿‡æ»¤çš„å™ªéŸ³æ¨¡å¼ (é’ˆå¯¹Qtå’Œç¼–è¯‘ä¸­é—´æ–‡ä»¶)
        self.ignore_patterns = [
            r'moc_.*\.cpp',      # Qtå…ƒå¯¹è±¡ç¼–è¯‘å™¨ç”Ÿæˆæ–‡ä»¶
            r'qrc_.*\.cpp',      # Qtèµ„æºç¼–è¯‘å™¨ç”Ÿæˆæ–‡ä»¶
            r'ui_.*\.h',         # Qtç•Œé¢ç”Ÿæˆæ–‡ä»¶
            r'build/',           # æ„å»ºç›®å½•
            r'cmake-build',      # CMakeæ„å»ºç›®å½•
            r'CMakeFiles/',
            r'\.g\.',            # Goç”Ÿæˆæ–‡ä»¶(å¦‚æœæœ‰)
            r'CMakeLists\.txt',  # æ„å»ºè„šæœ¬
            r'Makefile'
        ]
        
        # ğŸ†• æ–°å¢ï¼šå®šä¹‰éœ€è¦å¿½ç•¥çš„ç‰¹å®šé”™è¯¯æ¶ˆæ¯ (ç¯å¢ƒé…ç½®ç›¸å…³å™ªéŸ³)
        self.ignore_messages = [
            "file not found",           # ç¼ºå°‘å¤´æ–‡ä»¶å¯¼è‡´çš„é”™è¯¯
            "unknown type",             # ç±»å‹æ¨å¯¼å¤±è´¥
            "ConfigurationNotChecked",  # Cppchecké…ç½®è·³è¿‡è­¦å‘Š
            "clang-diagnostic-error",   # Clangç¼–è¯‘ç¯å¢ƒé”™è¯¯
            "too many errors emitted"   # é”™è¯¯è¿‡å¤šæç¤º
        ]

    def parse_and_merge(
        self, 
        tool_results: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """è§£æå¹¶åˆå¹¶å¤šä¸ªå·¥å…·çš„ç»“æœ"""
        try:
            all_issues = []
            tool_summaries = {}
            
            for tool_name, result in tool_results.items():
                if not isinstance(result, dict) or not result.get('success', False):
                    # å®¹é”™å¤„ç†ï¼šå¦‚æœæ˜¯Exceptionå¯¹è±¡æˆ–è€…success=False
                    error_msg = str(result.get('error', 'Unknown error')) if isinstance(result, dict) else str(result)
                    log_error(f"å·¥å…· {tool_name} åˆ†æå¤±è´¥æˆ–è·³è¿‡: {error_msg}")
                    tool_summaries[tool_name] = {
                        'success': False,
                        'issues_count': 0,
                        'error': error_msg
                    }
                    continue
                
                issues = result.get('issues', [])
                # ğŸ†• åœ¨è§£ææ—¶ç›´æ¥è¿‡æ»¤å™ªéŸ³
                parsed_issues = self._parse_tool_issues(tool_name, issues)
                
                # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿›è¡Œè¿›ä¸€æ­¥çš„ä¸šåŠ¡é€»è¾‘è¿‡æ»¤ (ä¿ç•™åŸæœ‰é€»è¾‘)
                if context:
                    parsed_issues = self._filter_issues_by_context(parsed_issues, context)
                
                all_issues.extend(parsed_issues)
                
                tool_summaries[tool_name] = {
                    'success': True,
                    'issues_count': len(parsed_issues)
                }
                
                log_info(f"è§£æ {tool_name} ç»“æœ: {len(issues)} -> {len(parsed_issues)} (è¿‡æ»¤å)")
            
            # å»é‡å’Œæ’åº
            deduplicated_issues = self._deduplicate_issues(all_issues)
            sorted_issues = self._sort_issues_by_priority(deduplicated_issues)
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            statistics = self._generate_statistics(sorted_issues)
            
            return {
                'total_issues': len(sorted_issues),
                'issues': sorted_issues,
                'statistics': statistics,
                'tool_summaries': tool_summaries
            }
            
        except Exception as e:
            log_error(f"ç»“æœè§£æå¤±è´¥: {str(e)}")
            import traceback
            log_error(traceback.format_exc())
            return {
                'total_issues': 0,
                'issues': [],
                'statistics': {},
                'tool_summaries': {},
                'error': str(e)
            }
    
    def _is_noise(self, file_path: str, message: str) -> bool:
        """ğŸ†• åˆ¤æ–­æ˜¯å¦ä¸ºå™ªéŸ³æ•°æ®"""
        # 1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„é»‘åå•
        for pattern in self.ignore_patterns:
            if re.search(pattern, file_path, re.IGNORECASE):
                return True
                
        # 2. æ£€æŸ¥é”™è¯¯æ¶ˆæ¯é»‘åå•
        msg_lower = message.lower()
        for ignore_msg in self.ignore_messages:
            if ignore_msg.lower() in msg_lower:
                return True
                
        # 3. è¿‡æ»¤ç³»ç»Ÿç»å¯¹è·¯å¾„æŠ¥é”™ (å¦‚ /usr/include, /opt)
        # æˆ‘ä»¬åªå…³å¿ƒç”¨æˆ·ä¸Šä¼ ç›®å½•ä¸‹çš„ä»£ç 
        if file_path.startswith('/') and 'uploads' not in file_path:
             # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„åˆ¤æ–­ï¼Œå¦‚æœä¸æ˜¯åŒ…å«åœ¨æˆ‘ä»¬çš„å·¥ä½œç›®å½•é‡Œï¼Œå¯èƒ½æ˜¯ç³»ç»Ÿåº“
             if file_path.startswith('/usr/') or file_path.startswith('/opt/'):
                 return True
            
        return False

    def _parse_tool_issues(self, tool_name: str, issues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è§£æç‰¹å®šå·¥å…·çš„é—®é¢˜åˆ—è¡¨"""
        parsed_issues = []
        
        for i, issue_data in enumerate(issues):
            try:
                file_path = issue_data.get('file', 'unknown')
                message = issue_data.get('message', '')
                
                # ğŸ†• è¿‡æ»¤é€»è¾‘å…¥å£
                if self._is_noise(file_path, message):
                    continue

                # å¤„ç† Flawfinder çš„æ•°å€¼å‹ severity
                raw_severity = issue_data.get('severity', 'info')
                
                parsed_issue = {
                    'id': f"{tool_name}_{i}_{hash(str(issue_data)) % 10000}",
                    'file': file_path,
                    'line': issue_data.get('line', 0),
                    'column': issue_data.get('column'),
                    'severity': self._normalize_severity(raw_severity),
                    'category': issue_data.get('category', 'code_quality'), # é»˜è®¤ç±»åˆ«
                    'message': message,
                    'tool': tool_name
                }
                parsed_issues.append(parsed_issue)
                
            except Exception as e:
                log_error(f"è§£æé—®é¢˜å¤±è´¥ {tool_name}: {str(e)}")
                continue
        
        return parsed_issues
    
    def _filter_issues_by_context(
        self, 
        issues: List[Dict[str, Any]], 
        context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ ¹æ®ä¸Šä¸‹æ–‡è¿‡æ»¤è¯¯æŠ¥ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰"""
        platform_info = context.get('platform_info', {})
        detected_platforms = platform_info.get('detected_platforms', [])
        
        filtered_issues = []
        for issue in issues:
            message_lower = issue.get('message', '').lower()
            
            if 'windows' in detected_platforms:
                # è·³è¿‡Linuxç‰¹å®šçš„è­¦å‘Š
                if any(keyword in message_lower for keyword in ['pthread', 'fork', 'sbrk']):
                    continue
            
            filtered_issues.append(issue)
        
        return filtered_issues
    
    def _normalize_severity(self, severity: Any) -> str:
        """æ ‡å‡†åŒ–ä¸¥é‡ç¨‹åº¦ (å¢å¼ºç‰ˆï¼Œæ”¯æŒæ•°å­—)"""
        # å¤„ç† Flawfinder çš„æ•°å­—ç­‰çº§ (1-5)
        if isinstance(severity, int) or (isinstance(severity, str) and severity.isdigit()):
            level = int(severity)
            if level >= 4: return 'critical'
            if level == 3: return 'high'
            if level == 2: return 'medium'
            return 'low'
            
        severity_lower = str(severity).lower()
        return self.severity_map.get(severity_lower, 'medium')
    
    def _deduplicate_issues(self, issues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é™¤é‡å¤çš„é—®é¢˜ (å¢å¼ºç‰ˆï¼šå…è®¸ä¸åŒå·¥å…·æŠ¥åŒä¸€è¡Œï¼Œä½†å»é™¤å®Œå…¨é‡å¤é¡¹)"""
        seen = set()
        deduplicated = []
        
        for issue in issues:
            # Key: æ–‡ä»¶ + è¡Œå· + å·¥å…· + æ¶ˆæ¯æ‘˜è¦
            # è¿™æ ·å¦‚æœä¸¤ä¸ªå·¥å…·éƒ½æŠ¥äº†åŒä¸€è¡Œï¼Œæˆ‘ä»¬éƒ½ä¿ç•™ï¼ˆå› ä¸ºè§†è§’ä¸åŒï¼‰
            # ä½†å¦‚æœåŒä¸€ä¸ªå·¥å…·å¯¹åŒä¸€è¡ŒæŠ¥äº†ä¸¤æ¬¡ä¸€æ ·çš„ï¼Œå°±å»é‡
            key = (issue['file'], issue['line'], issue['tool'], issue['message'][:50])
            
            if key not in seen:
                seen.add(key)
                deduplicated.append(issue)
        
        return deduplicated
    
    def _sort_issues_by_priority(self, issues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æŒ‰ä¼˜å…ˆçº§æ’åºé—®é¢˜"""
        severity_priority = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3, 'info': 4}
        
        return sorted(issues, key=lambda x: (
            severity_priority.get(x['severity'], 4),
            x['file'],
            x['line']
        ))
    
    def _generate_statistics(self, issues: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'severity_distribution': {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0},
            'category_distribution': {},
            'file_distribution': {},
            'tool_distribution': {}
        }
        
        for issue in issues:
            sev = issue['severity']
            # å®¹é”™ï¼šå¦‚æœsevä¸åœ¨é»˜è®¤keyé‡Œï¼Œè®¾ä¸ºmedium
            if sev not in stats['severity_distribution']:
                sev = 'medium'
            stats['severity_distribution'][sev] += 1
            
            category = issue['category']
            stats['category_distribution'][category] = stats['category_distribution'].get(category, 0) + 1
            
            file_path = issue['file']
            stats['file_distribution'][file_path] = stats['file_distribution'].get(file_path, 0) + 1
            
            tool = issue['tool']
            stats['tool_distribution'][tool] = stats['tool_distribution'].get(tool, 0) + 1
        
        return stats
