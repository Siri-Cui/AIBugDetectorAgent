# -*- coding: utf-8 -*-
"""
GLM-4 API å®¢æˆ·ç«¯ï¼ˆè¿­ä»£5å¢å¼ºç‰ˆï¼šæ”¯æŒåŠ¨æ€åˆ†æåå¤„ç†ï¼‰
ä½œç”¨ï¼šé›†æˆæ™ºè°± AI GLM-4 å¤§æ¨¡å‹ï¼Œæä¾› AI åˆ†æèƒ½åŠ›ï¼ˆæ”¯æŒé•¿ä¸Šä¸‹æ–‡ã€ä¸“é¡¹æ¨¡æ¿ã€åŠ¨æ€åˆ†æå»é‡ï¼‰
ä¾èµ–ï¼šzhipuaiã€config.settingsã€utils.logger
è°ƒç”¨å…³ç³»ï¼š
  1. è¢« repair_generator_agent è°ƒç”¨ï¼ˆé™æ€åˆ†æä¿®å¤å»ºè®®ï¼‰
  2. è¢« ai_postprocessor è°ƒç”¨ï¼ˆåŠ¨æ€åˆ†ææ™ºèƒ½å»é‡+åˆ†æï¼‰
"""

from __future__ import annotations

from typing import Dict, List, Any, Optional
from zhipuai import ZhipuAI

from utils.logger import log_info, log_error
from config import settings


class LLMClient:
    """GLM-4 å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆè¿­ä»£5å¢å¼ºç‰ˆï¼‰"""

    def __init__(self) -> None:
        self.client: Optional[ZhipuAI] = None
        self.model_name: str = getattr(settings, "MODEL_NAME", "glm-4-plus")  # ğŸ”¥ æ¨èç”¨ glm-4-plus
        self.api_key: Optional[str] = getattr(settings, "ZHIPU_API_KEY", None)
        self._initialize_client()

    def _initialize_client(self) -> None:
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        try:
            if self.api_key:
                self.client = ZhipuAI(api_key=self.api_key)
                log_info(f"GLM-4 å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆæ¨¡å‹: {self.model_name}ï¼‰")
            else:
                log_error("GLM-4 API å¯†é’¥æœªé…ç½®ï¼ˆZHIPU_API_KEY ä¸ºç©ºï¼‰")
        except Exception as e:
            log_error(f"GLM-4 å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e!s}")

    # ========== åŸæœ‰æ–¹æ³•1: é™æ€åˆ†æçš„æ‰¹é‡ä¿®å¤å»ºè®®ï¼ˆä¿æŒä¸å˜ï¼‰==========
    async def analyze_code_issues(
        self,
        issues: List[Dict[str, Any]],
        project_context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        åˆ†æä»£ç é—®é¢˜å¹¶ç”Ÿæˆæ™ºèƒ½å»ºè®®ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼Œç”¨äºæ‰¹é‡åˆ†æï¼‰

        :param issues: æ ‡å‡†åŒ–é—®é¢˜åˆ—è¡¨ï¼ˆå«å­—æ®µï¼šcategory/message/line/severity ç­‰ï¼‰
        :param project_context: é¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        :return: { success: bool, analysis?: {...}, raw_response?: str, error?: str }
        """
        try:
            if not self.client:
                return {"success": False, "error": "GLM-4 client not initialized"}

            # ä»…å–é«˜/ä¸­å±é—®é¢˜ï¼Œæœ€å¤š 6 æ¡
            critical_issues = [
                it for it in (issues or []) if it.get("severity") in {"high", "medium"}
            ][:6]

            if not critical_issues:
                return {
                    "success": True,
                    "analysis": {
                        "recommendations": ["æœªå‘ç°é«˜/ä¸­å±é—®é¢˜ï¼Œæ— éœ€ AI ä¿®å¤å»ºè®®ã€‚"],
                        "summary": "æ— é«˜/ä¸­å±é—®é¢˜",
                    },
                    "raw_response": "",
                }

            prompt = self._build_code_fix_prompt(critical_issues)

            log_info(f"å‘ GLM-4 å‘é€ä»£ç ä¿®å¤è¯·æ±‚ï¼Œé—®é¢˜æ•°é‡: {len(critical_issues)}")

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=2500,
            )

            content = (response.choices[0].message.content or "").strip()

            log_info("GLM-4 ä»£ç ä¿®å¤åˆ†æå®Œæˆ")

            return {
                "success": True,
                "analysis": {
                    "recommendations": [content],
                    "summary": "å·²ç”Ÿæˆå…·ä½“çš„ä»£ç ä¿®å¤æ–¹æ¡ˆ",
                },
                "raw_response": content,
            }

        except Exception as e:
            log_error(f"GLM-4 åˆ†æå¼‚å¸¸: {e!s}")
            return {"success": False, "error": str(e)}

    def _build_code_fix_prompt(self, issues: List[Dict[str, Any]]) -> str:
        """
        æ„å»ºä»£ç ä¿®å¤ä¸“ç”¨æç¤ºè¯ï¼ˆåŒ…å«æ ¼å¼åŒ–è¾“å‡ºè¦æ±‚ä¸ç¤ºä¾‹ï¼‰
        """
        header = (
            "ä½ æ˜¯ä¸€ä½ C++ ä»£ç å®‰å…¨ä¿®å¤ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä»£ç é—®é¢˜æä¾›å…·ä½“çš„ä¿®å¤æ–¹æ¡ˆï¼Œ"
            "å¿…é¡»åŒ…å«å¯ç›´æ¥ä½¿ç”¨çš„ä¿®å¤ä»£ç ã€‚\n\n## éœ€è¦ä¿®å¤çš„é—®é¢˜ï¼š\n"
        )

        parts: List[str] = [header]
        for i, issue in enumerate(issues[:6], 1):
            category = issue.get("category", "unknown")
            message = issue.get("message", "")
            line = issue.get("line", 0)
            severity = (issue.get("severity") or "unknown").upper()

            parts.append(
                f"\n**é—®é¢˜ {i}: {category}**\n"
                f"- ä½ç½®ï¼šç¬¬ {line} è¡Œ\n"
                f"- ä¸¥é‡ç¨‹åº¦ï¼š{severity}\n"
                f"- é—®é¢˜æè¿°ï¼š{message}\n"
            )

        tail = (
            "\n## è¾“å‡ºè¦æ±‚ï¼š\n\n"
            "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é€æ¡æä¾›ä¿®å¤æ–¹æ¡ˆï¼ˆé—®é¢˜ç¼–å·éœ€ä¸ä¸Šæ–¹ä¸€è‡´ï¼‰ï¼š\n\n"
            "### é—®é¢˜1: [é—®é¢˜ç±»å‹] (ç¬¬Xè¡Œ)\n\n"
            "**åŸå§‹ä»£ç é—®é¢˜ï¼š**\n"
            "```cpp\n"
            "// åœ¨æ­¤å±•ç¤ºæœ‰é—®é¢˜çš„ä»£ç ï¼ˆå¿…è¦æ—¶å¯ä¼ªä»£ç è¿˜åŸåœºæ™¯ï¼‰\n"
            "```\n\n"
            "**ä¿®å¤åçš„ä»£ç ï¼š**\n\n"
            "```cpp\n"
            "// æä¾›å®Œæ•´å¯ç¼–è¯‘çš„ä¿®å¤ä»£ç ï¼ŒåŒ…å«å¿…è¦çš„å¤´æ–‡ä»¶ä¸è¾¹ç•Œæ£€æŸ¥\n"
            "```\n\n"
            "**ä¿®å¤è¯´æ˜ï¼š**\n"
            "ç®€è¦è§£é‡Šä¿®å¤åŸç†ã€è¾¹ç•Œæ¡ä»¶ä¸æ³¨æ„äº‹é¡¹ï¼ˆä¸­æ–‡è¯´æ˜ï¼Œä»£ç è‹±æ–‡æ³¨é‡Šï¼‰ã€‚\n\n"
            "------\n\n"
            "### é—®é¢˜2: [é—®é¢˜ç±»å‹] (ç¬¬Yè¡Œ)\n\n"
            "**åŸå§‹ä»£ç é—®é¢˜ï¼š**\n"
            "```cpp\n"
            "// åœ¨æ­¤å±•ç¤ºæœ‰é—®é¢˜çš„ä»£ç \n"
            "```\n\n"
            "**ä¿®å¤åçš„ä»£ç ï¼š**\n"
            "```cpp\n"
            "// æä¾›å®Œæ•´å¯ç”¨çš„ä¿®å¤ä»£ç \n"
            "```\n\n"
            "**ä¿®å¤è¯´æ˜ï¼š**\n"
            "è¯¦ç»†è§£é‡Šä¿®å¤åŸç†ã€‚\n\n"
            "------\n\n"
            "## æ€»ä½“å»ºè®®ï¼š\n\n"
            "1. ä¿®å¤ä¼˜å…ˆçº§æ’åº\n"
            "2. ä»£ç è´¨é‡æ”¹è¿›å»ºè®®\n"
            "3. é¢„é˜²ç±»ä¼¼é—®é¢˜çš„æœ€ä½³å®è·µ\n\n"
            "**é‡è¦è¦æ±‚ï¼š**\n\n"
            "- å¿…é¡»æä¾›å®Œæ•´å¯ç¼–è¯‘çš„ C++ ä»£ç \n"
            "- æ¯ä¸ªä¿®å¤æ–¹æ¡ˆéƒ½è¦åŒ…å«å…·ä½“çš„ä»£ç ç¤ºä¾‹\n"
            "- è§£é‡Šä¿®å¤åŸç†å’Œæ³¨æ„äº‹é¡¹\n"
            "- ç”¨ä¸­æ–‡å›ç­”ï¼Œä»£ç ç”¨è‹±æ–‡æ³¨é‡Š\n"
            "- ä»…è¾“å‡ºä¿®å¤æ‰€éœ€çš„å†…å®¹ï¼Œä¸è¦é¢å¤–å‘æŒ¥æ— å…³å†…å®¹\n"
        )

        parts.append(tail)
        return "".join(parts)

    # ========== åŸæœ‰æ–¹æ³•2: é•¿ä¸Šä¸‹æ–‡åˆ†æï¼ˆä¿æŒå…¼å®¹ï¼Œç¨ä½œå¢å¼ºï¼‰==========
    async def analyze_with_long_context(
        self,
        prompt: str,
        max_tokens: int = 2000,
        temperature: float = 0.2
    ) -> Dict[str, Any]:
        """
        æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„LLMåˆ†æï¼ˆç”¨äºä¼ å…¥å®Œæ•´å‡½æ•°ä½“ï¼‰
        
        :param prompt: å®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«çœŸå®ä»£ç ï¼‰
        :param max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
        :param temperature: æ¸©åº¦å‚æ•°
        :return: { success: bool, content?: str, error?: str }
        """
        try:
            if not self.client:
                return {"success": False, "error": "GLM-4 client not initialized"}

            log_info("è°ƒç”¨ GLM-4 è¿›è¡Œé•¿ä¸Šä¸‹æ–‡åˆ†æ")

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )

            content = (response.choices[0].message.content or "").strip()

            log_info(f"GLM-4 é•¿ä¸Šä¸‹æ–‡åˆ†æå®Œæˆï¼Œè¿”å›é•¿åº¦: {len(content)}")

            return {
                "success": True,
                "content": content
            }

        except Exception as e:
            log_error(f"GLM-4 é•¿ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e!s}")
            return {"success": False, "error": str(e)}

    # ğŸ†•ğŸ†•ğŸ†• ========== æ–°å¢æ–¹æ³•: åŠ¨æ€åˆ†ææ™ºèƒ½åå¤„ç†ä¸“ç”¨ ==========
    async def analyze_with_context(
        self,
        prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 8000
    ) -> str:
        """
        ğŸ†• åŠ¨æ€åˆ†ææ™ºèƒ½åå¤„ç†ä¸“ç”¨æ–¹æ³•ï¼ˆç”¨äºå»é‡+åˆ†æ+ä¿®å¤å»ºè®®ï¼‰
        
        ä¸ analyze_with_long_context çš„åŒºåˆ«:
          - ç›´æ¥è¿”å› str (ä¸åŒ…è£…æˆ dict)
          - é»˜è®¤æ›´é«˜çš„ max_tokens (8000)
          - ä¸“ç”¨äº ai_postprocessor çš„ JSON å“åº”è§£æ
        
        :param prompt: å®Œæ•´çš„åˆ†ææç¤ºè¯ï¼ˆåŒ…å«issues+æºç ï¼‰
        :param temperature: æ¸©åº¦å‚æ•°(0.0-1.0,è¶Šä½è¶Šç¨³å®š)
        :param max_tokens: æœ€å¤§è¿”å›tokenæ•°
        :return: AIè¿”å›çš„åŸå§‹æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
        :raises Exception: è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            if not self.client:
                raise RuntimeError("GLM-4 å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

            log_info(f"ğŸ“¡ è°ƒç”¨æ™ºè°±AIè¿›è¡ŒåŠ¨æ€åˆ†æåå¤„ç† (temperature={temperature}, max_tokens={max_tokens})")

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„C/C++é™æ€åˆ†æä¸“å®¶,æ“…é•¿è¯†åˆ«å®‰å…¨æ¼æ´å¹¶æä¾›ä¿®å¤å»ºè®®ã€‚"
                            "ä½ çš„å›ç­”å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼,ä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæ–‡å­—ã€‚"
                        )
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.7
            )

            result = (response.choices[0].message.content or "").strip()

            log_info(f"âœ… AIè¿”å›äº† {len(result)} ä¸ªå­—ç¬¦")

            return result

        except Exception as e:
            log_error(f"âŒ GLM-4 åŠ¨æ€åˆ†æåå¤„ç†å¤±è´¥: {e}")
            raise

    # ========== å…¼å®¹æ€§æ£€æŸ¥æ–¹æ³•ï¼ˆå¯é€‰ï¼‰==========
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return self.client is not None
